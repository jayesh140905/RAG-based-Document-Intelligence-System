{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14627818,"datasetId":9343902,"databundleVersionId":15465778}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install -q pymupdf pdfplumber sentence-transformers faiss-cpu tiktoken accelerate bitsandbytes gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T11:05:39.335703Z","iopub.execute_input":"2026-01-30T11:05:39.335945Z","iopub.status.idle":"2026-01-30T11:05:53.453352Z","shell.execute_reply.started":"2026-01-30T11:05:39.335924Z","shell.execute_reply":"2026-01-30T11:05:53.452655Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0rc2 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport re\nimport torch\nimport fitz  # PyMuPDF\nimport faiss\nimport numpy as np\nimport gradio as gr\nfrom typing import List, Dict\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# Configuration\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"âœ… Running on: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-30T11:05:53.455364Z","iopub.execute_input":"2026-01-30T11:05:53.455600Z","iopub.status.idle":"2026-01-30T11:06:29.874474Z","shell.execute_reply.started":"2026-01-30T11:05:53.455576Z","shell.execute_reply":"2026-01-30T11:06:29.873639Z"}},"outputs":[{"name":"stderr","text":"2026-01-30 11:06:12.831563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769771173.013478      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769771173.066223      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769771173.517723      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769771173.517761      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769771173.517764      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769771173.517767      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"âœ… Running on: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class PDFRagEngine:\n    def __init__(self, embedding_model_name=\"BAAI/bge-base-en-v1.5\"):\n        # 1. Load Embedding Model (for Retrieval)\n        print(\"Loading Embedding Model...\")\n        self.embedder = SentenceTransformer(embedding_model_name, device=device)\n        \n        # 2. Load Generation Model (The LLM)\n        # SWITCHED TO TinyLlama: No login required, runs fast on Kaggle\n        print(\"Loading LLM (TinyLlama-1.1B)...\")\n        self.llm_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n        self.tokenizer = AutoTokenizer.from_pretrained(self.llm_model_id)\n        self.llm = AutoModelForCausalLM.from_pretrained(\n            self.llm_model_id,\n            device_map=\"auto\",\n            torch_dtype=torch.float16\n        )\n        \n        # Internal storage\n        self.chunks = []\n        self.index = None\n        \n    def _clean_text(self, text: str) -> str:\n        \"\"\"Internal helper to clean extracted text.\"\"\"\n        text = re.sub(r'\\n+', ' ', text) \n        text = re.sub(r'\\s+', ' ', text) \n        return text.strip()\n\n    def ingest_pdf(self, pdf_path: str):\n        \"\"\"extracts text, chunks it, and builds the vector index.\"\"\"\n        doc = fitz.open(pdf_path)\n        raw_text = \"\"\n        \n        for page in doc:\n            raw_text += page.get_text()\n            \n        cleaned_text = self._clean_text(raw_text)\n        chunk_size = 500\n        overlap = 50\n        \n        self.chunks = [\n            cleaned_text[i : i + chunk_size] \n            for i in range(0, len(cleaned_text), chunk_size - overlap)\n        ]\n        print(f\"âœ… PDF Processed: {len(self.chunks)} chunks created.\")\n        \n        embeddings = self.embedder.encode(\n            self.chunks, \n            convert_to_numpy=True, \n            normalize_embeddings=True,\n            show_progress_bar=True\n        )\n        \n        dimension = embeddings.shape[1]\n        self.index = faiss.IndexFlatIP(dimension)\n        self.index.add(embeddings)\n        print(\"âœ… Vector Index Built.\")\n\n    def retrieve(self, query: str, top_k=3):\n        \"\"\"Finds the most relevant chunks for a query.\"\"\"\n        query_emb = self.embedder.encode(\n            [f\"Represent this question for retrieving relevant passages: {query}\"], \n            convert_to_numpy=True, \n            normalize_embeddings=True\n        )\n        scores, indices = self.index.search(query_emb, top_k)\n        results = [self.chunks[idx] for idx in indices[0]]\n        return results\n\n    def generate_answer(self, query: str):\n        \"\"\"Retrieves context and generates an answer using the LLM.\"\"\"\n        context_chunks = self.retrieve(query)\n        context_text = \"\\n\\n\".join(context_chunks)\n        \n        # TinyLlama Chat Prompt Format\n        prompt = f\"\"\"<|system|>\nYou are a helpful assistant. Use the provided context to answer the question. If the answer is not in the context, say \"I don't know\".</s>\n<|user|>\nContext:\n{context_text}\n\nQuestion:\n{query}</s>\n<|assistant|>\n\"\"\"\n        \n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n        \n        outputs = self.llm.generate(\n            **inputs, \n            max_new_tokens=200, \n            temperature=0.7,\n            do_sample=True \n        )\n        \n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        # Extract only the assistant's part\n        final_answer = response.split(\"<|assistant|>\")[-1].strip()\n        return final_answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T11:06:29.875437Z","iopub.execute_input":"2026-01-30T11:06:29.876218Z","iopub.status.idle":"2026-01-30T11:06:29.886930Z","shell.execute_reply.started":"2026-01-30T11:06:29.876178Z","shell.execute_reply":"2026-01-30T11:06:29.886212Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Initialize the engine (this takes about 30-60 seconds to load models)\nrag_engine = PDFRagEngine()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T11:06:29.887931Z","iopub.execute_input":"2026-01-30T11:06:29.888259Z","iopub.status.idle":"2026-01-30T11:06:42.516220Z","shell.execute_reply.started":"2026-01-30T11:06:29.888235Z","shell.execute_reply":"2026-01-30T11:06:42.515344Z"}},"outputs":[{"name":"stdout","text":"Loading Embedding Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"509c7db61e174aedaad30484cff72187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d984321567fa4688b517bbc5667c11ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f57758091684161be932affe220b3d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43da53d985374056a2481fa8dc139bff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e07d05fc0bc46ad9c39ddf1796b48dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a2f4a4c1714c8b95abb9bd122cb2a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4993438ada6a46c381ac83b9c79f0808"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a6320331da64d439ba5572ce4391dfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f114bac3d040feb2cd2a8580bb49e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb8744a920224e76b7e7d050df8a8515"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5de0a5a821d48a79895a79e0afa3da7"}},"metadata":{}},{"name":"stdout","text":"Loading LLM (TinyLlama-1.1B)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0e482d7a254c63a5780acb3ea4b613"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5d32ba850c641c3b0a25f56e2a48aeb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"308b1989ba85475ab93e686db36ec0e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fe9b19dd98a4406a59389dda8292afb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5becefa6551432cb6346d3fa3d950fc"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa8f465e65d4d2897386d1a332f6c76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c73c925767d428186036cc422cee146"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Find your PDF automatically\ndata_dir = \"/kaggle/input\"\npdf_path = None\n\nfor root, dirs, files in os.walk(data_dir):\n    for file in files:\n        if file.lower().endswith(\".pdf\"):\n            pdf_path = os.path.join(root, file)\n            break\n\nif pdf_path:\n    print(f\"ðŸ“„ Found PDF: {pdf_path}\")\n    rag_engine.ingest_pdf(pdf_path)\nelse:\n    print(\"âŒ No PDF found! Please upload one to Kaggle inputs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T11:06:42.518018Z","iopub.execute_input":"2026-01-30T11:06:42.518299Z","iopub.status.idle":"2026-01-30T11:06:43.383883Z","shell.execute_reply.started":"2026-01-30T11:06:42.518273Z","shell.execute_reply":"2026-01-30T11:06:43.383231Z"}},"outputs":[{"name":"stdout","text":"ðŸ“„ Found PDF: /kaggle/input/pdfone-1/DL.pdf\nâœ… PDF Processed: 46 chunks created.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"171e1b80e23a411b994e2242d53b36d1"}},"metadata":{}},{"name":"stdout","text":"âœ… Vector Index Built.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"question = \"What is the main methodology proposed in this paper?\"\n\n# Calling the public method of our class\nanswer = rag_engine.generate_answer(question)\n\nprint(\"-\" * 50)\nprint(f\"â“ Question: {question}\")\nprint(f\"ðŸ’¡ Answer: {answer}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T11:06:43.384731Z","iopub.execute_input":"2026-01-30T11:06:43.384992Z","iopub.status.idle":"2026-01-30T11:06:49.432363Z","shell.execute_reply.started":"2026-01-30T11:06:43.384966Z","shell.execute_reply":"2026-01-30T11:06:49.431553Z"}},"outputs":[{"name":"stdout","text":"--------------------------------------------------\nâ“ Question: What is the main methodology proposed in this paper?\nðŸ’¡ Answer: The main methodology proposed in this paper is to use a transformer-based co-attention mechanism to combine the commentator's text and structured event data to align words from the commentary with actual events happening on the field. The paper is an example of multi-task learning (MTL) and is trained as a MTL system to perform three tasks simultaneously: correctness classification, emotion prediction, and sentiment analysis. The qualitative examples in Table V confirm the quantitative results.\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def process_query(message, history):\n    return rag_engine.generate_answer(message)\n\n# Simple Chat Interface\ndemo = gr.ChatInterface(\n    fn=process_query, \n    title=\"ðŸ“š NotebookLM Clone (Powered by Gemma-2b)\",\n    description=\"Ask questions about your PDF. The AI reads the document and answers based *only* on the text.\"\n)\n\ndemo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T11:06:49.433489Z","iopub.execute_input":"2026-01-30T11:06:49.434209Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n  self.chatbot = Chatbot(\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://60c8e965f986469b52.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://60c8e965f986469b52.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":null}]}